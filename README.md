# Analyse-de-sentiment-multimodal

### **Description**
Ce projet combine plusieurs modalitÃ©s (vidÃ©o, audio et texte) pour prÃ©dire le sentiment en utilisant des modÃ¨les d'apprentissage profond, inspirÃ©s de DEVA. L'objectif est d'analyser les Ã©motions exprimÃ©es dans les donnÃ©es multimÃ©dia en fusionnant les signaux textuels, audio et visuels. Le systÃ¨me utilise des modÃ¨les basÃ©s sur BERT pour l'encodage du texte, OpenSMILE pour les caractÃ©ristiques audio et OpenFace pour l'analyse des expressions faciales. Le dataset utilisÃ© et CMU - MOSI (Segmented)
# **PrÃ©requis**

*   Python 3.8+
*   PyTorch
*   Transformers (HuggingFace)
*   OpenSMILE (pour l'extraction des caractÃ©ristiques audio)
*  OpenFace (pour l'analyse des expressions faciales)
*   Librosa (pour le traitement audio)
*   MoviePy (pour l'extraction de l'audio Ã  partir des vidÃ©os)
*   Pandas, Numpy (pour la manipulation des donnÃ©es)
*   Ã‰lÃ©ment de liste

**Structure du RÃ©pertoire**

 analyse_sentiment_multimodale.ipynb : ModÃ¨les entraÃ®nÃ©s et checkpoints.
 bert.py : la class BertTextEncoder


Installez les bibliothÃ¨ques requises avec pip :
```
pip install torch transformers librosa pandas numpy moviepy
```


### Dataset MOSI

Le **MOSI (Multimodal Opinion Sentiment Intensity)** est un dataset multimodal utilisÃ© pour la prÃ©diction des Ã©motions et des sentiments exprimÃ©s Ã  partir de donnÃ©es vidÃ©o, audio et textuelles. Le dataset MOSI contient des vidÃ©os de personnes exprimant des opinions sur des sujets variÃ©s, ainsi que les transcriptions textuelles et les caractÃ©ristiques audio associÃ©es.

Le lien vers le dataset : https://www.kaggle.com/datasets/mathurinache/cmu-mosi

#### Structure du dataset

Le dataset MOSI est composÃ© de plusieurs fichiers, principalement les suivants :
- **VidÃ©os** : Les vidÃ©os contiennent les expressions faciales et les mouvements corporels des individus exprimant leurs opinions.
- **Audio** : Les fichiers audio contiennent l'enregistrement des voix des personnes dans les vidÃ©os.
- **Textes** : Les transcriptions textuelles des vidÃ©os.


###  Fichier de Labels

Le **fichier de labels** contient les annotations des Ã©motions pour chaque vidÃ©o, accompagnÃ©es des identifiants des vidÃ©os et des segments correspondants. Ce fichier permet d'aligner les donnÃ©es multimodales (texte, audio, vidÃ©o) sur la mÃªme Ã©chelle d'annotation pour pouvoir entraÃ®ner un modÃ¨le de prÃ©diction des Ã©motions.

#### Structure du fichier `label.csv`

Le fichier `label.csv` contient les colonnes suivantes :
- **video_id** : Identifiant unique de la vidÃ©o.
- **clip_id** : Identifiant du segment vidÃ©o (peut correspondre Ã  une portion spÃ©cifique de la vidÃ©o).
- **text** : La transcription textuelle du segment vidÃ©o.
- **label** : Score d'Ã©motion allant de **-3 Ã  3**, reprÃ©sentant l'intensitÃ© de l'Ã©motion exprimÃ©e dans le texte de la vidÃ©o.
- **label_T**, **label_A**, **label_V** : Ã‰tiquettes spÃ©cifiques pour les donnÃ©es de texte, audio et vidÃ©o, respectivement. Ces Ã©tiquettes sont utilisÃ©es pour l'alignement des donnÃ©es multimodales.
- **annotation** : Annotation de l'Ã©motion, catÃ©gorisÃ©e comme **Positif**, **NÃ©gatif** ou **Neutre**.
- **mode** : Le mode de l'Ã©chantillon, typiquement **train** ou **test**, pour l'assignation Ã  l'ensemble d'entraÃ®nement ou de test.


# **PrÃ©traitement**
## **Traitement du video**
Pour l'analyse des expressions faciales, nous utilisons OpenFace pour extraire les unitÃ©s d'action faciale Ã  partir des images extraites des vidÃ©os.

Pour exÃ©cuter OpenFace, tÃ©lÃ©chargez et exÃ©cutez le logiciel :
TÃ©lÃ©chargez OpenFace depuis ici :https://sourceforge.net/projects/openface.mirror/

Extrayez et exÃ©cutez OpenFace dans le shell.
```
# DÃ©finir les chemins
$exePath = "E:\OpenFace_2.2.0_win_x86\OpenFace_2.2.0_win_x86\FeatureExtraction.exe"
$videoFolder = "E:\Raw\Video\Segmented"  # RÃ©pertoire contenant les vidÃ©os
$outDir = "E:\OpenFace_Results\processed"  # RÃ©pertoire de sortie

# CrÃ©er le rÃ©pertoire de sortie s'il n'existe pas dÃ©jÃ 
if (-not (Test-Path -Path $outDir)) {
    New-Item -Path $outDir -ItemType Directory
}

# RÃ©cupÃ©rer tous les fichiers vidÃ©o (formats .mp4, .avi, .mov)
$videoFiles = Get-ChildItem -Path $videoFolder -Recurse -File | Where-Object { $_.Extension -in @(".mp4", ".avi", ".mov") }

# Boucle sur chaque vidÃ©o et exÃ©cution de FeatureExtraction
foreach ($video in $videoFiles) {
    $videoPath = $video.FullName
    Write-Host "Processing: $videoPath"
    
    # ExÃ©cuter FeatureExtraction sur chaque vidÃ©o
    & $exePath -f $videoPath -out_dir $outDir
}

Write-Host "Traitement terminÃ© !"
```
Ã€ partir des fichiers .csv gÃ©nÃ©rÃ©s par OpenFace (une ligne par frame, une colonne par AU), nous :


1.   dÃ©tectons les Action Units (AUs) actives,
2.   sÃ©lectionnons les AUs les plus importantes,
3.   convertissons ces AUs en texte lisible,
4.   gÃ©nÃ©rons un identifiant unique pour chaque segment,
5.   sauvegardons lâ€™ensemble dans un fichier vision_text.pkl.

Ce fichier sera ensuite utilisÃ© dans la modalitÃ© Vision de notre modÃ¨le multimodal.

## **Traitement du audio**
Dans ce projet, nous extrayons l'audio des vidÃ©os Ã  l'aide de **MoviePy**, une bibliothÃ¨que Python qui permet de traiter et de manipuler les fichiers multimÃ©dia. Ensuite, nous utilisons **OpenSMILE**, un outil de traitement audio, pour extraire plusieurs caractÃ©ristiques importantes des fichiers audio, notamment :

- **Loudness** : Mesure de l'intensitÃ© du son.
- **Jitter** : Variation de la frÃ©quence fondamentale.
- **Shimmer** : Variation de l'amplitude du signal audio.
- **F0 (FrÃ©quence fondamentale)** : Valeur de la frÃ©quence fondamentale du signal.

Ces caractÃ©ristiques sont ensuite traitÃ©es et classÃ©es en trois niveaux (faible, normal, Ã©levÃ©) pour chaque dimension sonore. Ces descriptions sont ensuite converties en texte pour chaque vidÃ©o. Et puis on gÃ©nÃ¨re l'id pour chaque segment

Le code pour cette extraction et transformation des caractÃ©ristiques audio est prÃ©sent dans le notebook **`analyse_sentiment_multiomodal.ipynb`**, oÃ¹ chaque Ã©tape est dÃ©taillÃ©e et appliquÃ©e aux fichiers audio des vidÃ©os.

## **Traitement du Texte**
Nous traitons le texte avec BERT, en utilisant des modÃ¨les prÃ©-entraÃ®nÃ©s pour la tokenisation et la gÃ©nÃ©ration des embeddings. Ces embeddings sont utilisÃ©s pour prÃ©dire le sentiment du texte.

1. **Chargement des donnÃ©es**
   - Nous chargeons les fichiers **.pkl** contenant les textes bruts associÃ©s Ã  chaque segment vidÃ©o, audio ou visuel et les donnÃ©es textuel du dataset MOSI
   - Les fichiers pickle audio et video contiennent les textes associÃ©s aux segments audio et visuel respectivement.
 

2. **Tokenisation**
   - Les textes sont tokenisÃ©s Ã  lâ€™aide du **tokenizer BERT** (modÃ¨le prÃ©-entrainÃ© `bert-base-uncased`).
   - Chaque texte est transformÃ© en une sÃ©quence de tokens compatible avec BERT, avec un maximum de **128 tokens**.

3. **Encodage avec BERT**
   - Le texte tokenisÃ© est ensuite traitÃ© par le modÃ¨le **BERT**, qui gÃ©nÃ¨re des **embeddings** de taille 768 pour chaque token.
   - Ces embeddings sont ensuite utilisÃ©s dans le **TextEncoder**.

4. **Application du TextEncoder**
   - Le **TextEncoder** est appliquÃ© sur les embeddings de BERT. Il ajoute un token spÃ©cial **Eâ‚˜** pour indiquer le dÃ©but de la sÃ©quence de la modalitÃ©.
   - La sortie du **TextEncoder** est une sÃ©quence de **8 tokens** reprÃ©sentÃ©e par un vecteur de taille **768**.
   - **Seuls les 8 premiers tokens** gÃ©nÃ©rÃ©s par le **TextEncoder** sont utilisÃ©s pour l'entraÃ®nement, reprÃ©sentant ainsi les informations clÃ©s de chaque sÃ©quence.

5. **Fusion des embeddings**
   - AprÃ¨s avoir gÃ©nÃ©rÃ© les embeddings pour chaque modalitÃ© (texte, audio et vidÃ©o), les embeddings sont fusionnÃ©s pour Ãªtre utilisÃ©s dans le modÃ¨le multimodal.
  
   - ## **EntraÃ®nement du ModÃ¨le**
   ### Chargement des donnÃ©es

Les donnÃ©es utilisÃ©es dans ce projet sont stockÃ©es sous forme de fichiers `.pkl` pour les embeddings des trois modalitÃ©s : texte, audio et vidÃ©o. Ces fichiers sont chargÃ©s Ã  l'aide de la fonction `load_pkl()` qui lit les fichiers `.pkl` et rÃ©cupÃ¨re les embeddings ainsi que les identifiants (IDs) associÃ©s.

Les donnÃ©es sont ensuite extraites et prÃ©parÃ©es pour l'entraÃ®nement du modÃ¨le multimodal :

- **Texte** : Les embeddings textuels sont chargÃ©s .
- **Audio** : Les embeddings audio sont chargÃ©s `.
- **VidÃ©o** : Les embeddings vidÃ©o sont chargÃ©s `.

Les identifiants (IDs) associÃ©s Ã  chaque modalitÃ© sont Ã©galement extraits et utilisÃ©s pour l'alignement avec les IDs du fichier CSV des labels.

### Alignement des donnÃ©es multimodales

Les embeddings de chaque modalitÃ© (texte, audio et vidÃ©o) sont alignÃ©s avec les IDs correspondants Ã  partir du fichier CSV des labels. Un **projecteur d'embeddings** est utilisÃ© pour transformer chaque ensemble d'embeddings en sÃ©quences alignÃ©es, permettant ainsi d'avoir des donnÃ©es cohÃ©rentes pour l'entraÃ®nement du modÃ¨le.

La classe `AudioVisualFeatureProjector` permet d'aligner les embeddings audio, vidÃ©o et texte en utilisant les IDs du CSV. Les donnÃ©es sont ensuite alignÃ©es avec les IDs du CSV, garantissant que chaque Ã©chantillon de donnÃ©es multimodales (texte, audio, vidÃ©o) correspond Ã  une Ã©tiquette spÃ©cifique.

### CrÃ©ation du Dataset pour l'entraÃ®nement

Un dataset multimodal est crÃ©Ã© en combinant les embeddings alignÃ©s de chaque modalitÃ© avec les Ã©tiquettes issues du fichier CSV. Ce dataset est utilisÃ© pour entraÃ®ner, valider et tester le modÃ¨le.

Les donnÃ©es sont divisÃ©es en trois ensembles :

- **Ensemble d'entraÃ®nement** : 70% des donnÃ©es
- **Ensemble de validation** : 15% des donnÃ©es
- **Ensemble de test** : 15% des donnÃ©es

La division est rÃ©alisÃ©e Ã  l'aide de la fonction `train_test_split()` de Scikit-learn, permettant ainsi de sÃ©parer les donnÃ©es de maniÃ¨re alÃ©atoire tout en prÃ©servant la distribution des labels.

### Statistiques finales sur les donnÃ©es

AprÃ¨s avoir prÃ©parÃ© les donnÃ©es, nous vÃ©rifions les dimensions des embeddings et leur distribution sur les ensembles d'entraÃ®nement, de validation et de test. Les dimensions des embeddings sont affichÃ©es, et la rÃ©partition des labels (positifs et nÃ©gatifs) est Ã©galement vÃ©rifiÃ©e pour chaque ensemble.

Les rÃ©sultats des statistiques finales sont les suivants :

- Nombre d'Ã©chantillons dans chaque ensemble (train, val, test).
- Dimensions des embeddings pour chaque modalitÃ© (texte, audio, vidÃ©o).
- Distribution des labels dans chaque ensemble.


# DÃ©finir les chemins
$exePath = "E:\OpenFace_2.2.0_win_x86\OpenFace_2.2.0_win_x86\FeatureExtraction.exe"
$videoFolder = "E:\Raw\Video\Segmented"  # RÃ©pertoire contenant les vidÃ©os
$outDir = "E:\OpenFace_Results\processed"  # RÃ©pertoire de sortie

# CrÃ©er le rÃ©pertoire de sortie s'il n'existe pas dÃ©jÃ 
if (-not (Test-Path -Path $outDir)) {
    New-Item -Path $outDir -ItemType Directory
}

# RÃ©cupÃ©rer tous les fichiers vidÃ©o (formats .mp4, .avi, .mov)
$videoFiles = Get-ChildItem -Path $videoFolder -Recurse -File | Where-Object { $_.Extension -in @(".mp4", ".avi", ".mov") }

# Boucle sur chaque vidÃ©o et exÃ©cution de FeatureExtraction
foreach ($video in $videoFiles) {
    $videoPath = $video.FullName
    Write-Host "Processing: $videoPath"
    
    # ExÃ©cuter FeatureExtraction sur chaque vidÃ©o
    & $exePath -f $videoPath -out_dir $outDir
}

Write-Host "Traitement terminÃ© !"

Ã€ partir des fichiers .csv gÃ©nÃ©rÃ©s par OpenFace (une ligne par frame, une colonne par AU), nous :


1.   dÃ©tectons les Action Units (AUs) actives,
2.   sÃ©lectionnons les AUs les plus importantes,
3.   convertissons ces AUs en texte lisible,
4.   gÃ©nÃ©rons un identifiant unique pour chaque segment,
5.   sauvegardons lâ€™ensemble dans un fichier vision_text.pkl.

Ce fichier sera ensuite utilisÃ© dans la modalitÃ© Vision de notre modÃ¨le multimodal.

## **Traitement du audio**

Dans ce projet, nous extrayons l'audio des vidÃ©os Ã  l'aide de **MoviePy**, une bibliothÃ¨que Python qui permet de traiter et de manipuler les fichiers multimÃ©dia. Ensuite, nous utilisons **OpenSMILE**, un outil de traitement audio, pour extraire plusieurs caractÃ©ristiques importantes des fichiers audio, notamment :

- **Loudness** : Mesure de l'intensitÃ© du son.
- **Jitter** : Variation de la frÃ©quence fondamentale.
- **Shimmer** : Variation de l'amplitude du signal audio.
- **F0 (FrÃ©quence fondamentale)** : Valeur de la frÃ©quence fondamentale du signal.

Ces caractÃ©ristiques sont ensuite traitÃ©es et classÃ©es en trois niveaux (faible, normal, Ã©levÃ©) pour chaque dimension sonore. Ces descriptions sont ensuite converties en texte pour chaque vidÃ©o. Et puis on gÃ©nÃ¨re l'id pour chaque segment

Le code pour cette extraction et transformation des caractÃ©ristiques audio est prÃ©sent dans le notebook **`analyse_sentiment_multiomodal.ipynb`**, oÃ¹ chaque Ã©tape est dÃ©taillÃ©e et appliquÃ©e aux fichiers audio des vidÃ©os.

## **Traitement du Texte**
Nous traitons le texte avec BERT, en utilisant des modÃ¨les prÃ©-entraÃ®nÃ©s pour la tokenisation et la gÃ©nÃ©ration des embeddings. Ces embeddings sont utilisÃ©s pour prÃ©dire le sentiment du texte.

### Chargement des donnÃ©es

Les donnÃ©es utilisÃ©es dans ce projet sont stockÃ©es sous forme de fichiers `.pkl` pour les embeddings des trois modalitÃ©s : texte, audio et vidÃ©o. Ces fichiers sont chargÃ©s Ã  l'aide de la fonction `load_pkl()` qui lit les fichiers `.pkl` et rÃ©cupÃ¨re les embeddings ainsi que les identifiants (IDs) associÃ©s.

Les donnÃ©es sont ensuite extraites et prÃ©parÃ©es pour l'entraÃ®nement du modÃ¨le multimodal :

- **Texte** : Les embeddings textuels sont chargÃ©s .
- **Audio** : Les embeddings audio sont chargÃ©s `.
- **VidÃ©o** : Les embeddings vidÃ©o sont chargÃ©s `.

Les identifiants (IDs) associÃ©s Ã  chaque modalitÃ© sont Ã©galement extraits et utilisÃ©s pour l'alignement avec les IDs du fichier CSV des labels.

### Alignement des donnÃ©es multimodales

Les embeddings de chaque modalitÃ© (texte, audio et vidÃ©o) sont alignÃ©s avec les IDs correspondants Ã  partir du fichier CSV des labels. Un **projecteur d'embeddings** est utilisÃ© pour transformer chaque ensemble d'embeddings en sÃ©quences alignÃ©es, permettant ainsi d'avoir des donnÃ©es cohÃ©rentes pour l'entraÃ®nement du modÃ¨le.

La classe `AudioVisualFeatureProjector` permet d'aligner les embeddings audio, vidÃ©o et texte en utilisant les IDs du CSV. Les donnÃ©es sont ensuite alignÃ©es avec les IDs du CSV, garantissant que chaque Ã©chantillon de donnÃ©es multimodales (texte, audio, vidÃ©o) correspond Ã  une Ã©tiquette spÃ©cifique.

### CrÃ©ation du Dataset pour l'entraÃ®nement

Un dataset multimodal est crÃ©Ã© en combinant les embeddings alignÃ©s de chaque modalitÃ© avec les Ã©tiquettes issues du fichier CSV. Ce dataset est utilisÃ© pour entraÃ®ner, valider et tester le modÃ¨le.

Les donnÃ©es sont divisÃ©es en trois ensembles :

- **Ensemble d'entraÃ®nement** : 70% des donnÃ©es
- **Ensemble de validation** : 15% des donnÃ©es
- **Ensemble de test** : 15% des donnÃ©es

La division est rÃ©alisÃ©e Ã  l'aide de la fonction `train_test_split()` de Scikit-learn, permettant ainsi de sÃ©parer les donnÃ©es de maniÃ¨re alÃ©atoire tout en prÃ©servant la distribution des labels.

### Statistiques finales sur les donnÃ©es

AprÃ¨s avoir prÃ©parÃ© les donnÃ©es, nous vÃ©rifions les dimensions des embeddings et leur distribution sur les ensembles d'entraÃ®nement, de validation et de test. Les dimensions des embeddings sont affichÃ©es, et la rÃ©partition des labels (positifs et nÃ©gatifs) est Ã©galement vÃ©rifiÃ©e pour chaque ensemble.

Les rÃ©sultats des statistiques finales sont les suivants :

- Nombre d'Ã©chantillons dans chaque ensemble (train, val, test).
- Dimensions des embeddings pour chaque modalitÃ© (texte, audio, vidÃ©o).
- Distribution des labels dans chaque ensemble.

## ðŸ”¥ EntraÃ®nement du ModÃ¨le Multimodal (DEVANet)

Cette section dÃ©crit comment le modÃ¨le multimodal a Ã©tÃ© entraÃ®nÃ©, rÃ©gularisÃ© et Ã©valuÃ© aprÃ¨s lâ€™alignement des embeddings texteâ€“audioâ€“vidÃ©o.

---

### 1ï¸âƒ£ Normalisation des donnÃ©es

Avant lâ€™entraÃ®nement, les embeddings de chaque modalitÃ© (Texte, Audio, VidÃ©o) sont **normalisÃ©s** en utilisant :

- la **moyenne** et lâ€™**Ã©cart-type** des donnÃ©es dâ€™entraÃ®nement uniquement  
- une normalisation appliquÃ©e ensuite aux ensembles **train**, **validation** et **test**

Cette Ã©tape stabilise lâ€™entraÃ®nement et permet au modÃ¨le de converger plus rapidement.

---

### 2ï¸âƒ£ Dataset avec augmentation

Pour rendre le modÃ¨le plus robuste, une **augmentation lÃ©gÃ¨re** est appliquÃ©e pendant lâ€™entraÃ®nement :

- ajout dâ€™un bruit gaussien aux embeddings (- texte, audio, vidÃ©o -)
- probabilitÃ© de 50%
- standard deviation du bruit = **0.05**

âž¡ï¸ Cela simule des variations naturelles (bruit audio, micro-expression instable, variation textuelle).

---

### 3ï¸âƒ£ Architecture : Cross-Modal Attention

Le cÅ“ur du modÃ¨le repose sur une **attention croisÃ©e robuste** qui permet au texte dâ€™aller chercher des informations pertinentes dans :

- les embeddings **audio**
- les embeddings **vidÃ©o**

Lâ€™architecture utilisÃ©e comprend :

#### ðŸ”¹ RobustCrossModalAttention  
Un module d'attention qui calcule :
- Query (texte)
- Keys/Values (audio ou vidÃ©o)
- Matrice dâ€™attention + dropout

#### ðŸ”¹ SimplifiedMFU  
(Multimodal Fusion Unit simplifiÃ©e)

- effectue une double attention croisÃ©e Tâ†’A et Tâ†’V  
- rÃ©alise un **pooling temporel** sur les sÃ©quences  
- concatÃ¨ne les modalitÃ©s texte/audio/vidÃ©o  
- applique une couche fully connected + LayerNorm

#### ðŸ”¹ DEVANet RÃ©gularisÃ©

Le modÃ¨le final contient :

- **MFU** â†’ fusion multimodale
- **Classifier** â†’ MLP (2 couches) qui prÃ©dit le score dâ€™Ã©motion (-3 Ã  3)

RÃ©gularisation utilisÃ©e :
- Dropout = **0.4**
- Weight decay = **1e-3**
- Gradient clipping = **0.3**

---

### 4ï¸âƒ£ Fonction de perte hybride

Nous utilisons une **loss hybride** spÃ©cialement conÃ§ue pour les labels MOSI :

#### ðŸ”¸ MSE Loss  
Pour l'aspect continu : prÃ©diction du score dâ€™Ã©motion (-3 â†’ 3)

#### ðŸ”¸ BCE With Logits + Label Smoothing  
Pour la classification binaire :

- score > 0  â†’ **positif**
- score â‰¤ 0 â†’ **nÃ©gatif**

Label smoothing = **0.1**

âž¡ï¸ Cela stabilise lâ€™apprentissage lorsque les labels sont bruitÃ©s.

La loss finale :  
**0.5 Ã— MSE + 0.5 Ã— BCE_smooth**

---

### 5ï¸âƒ£ MÃ©triques dâ€™Ã©valuation

Comme MOSI est un dataset **continu**, mais souvent Ã©valuÃ© en binaire, nous utilisons :

| MÃ©trique | Description |
|---------|-------------|
| **Acc-2** | Accuracy binaire (score > 0 vs score â‰¤ 0) |
| **F1-Weighted** | Ã‰value lâ€™Ã©quilibre Positif/NÃ©gatif |
| **MAE** | Mesure lâ€™erreur absolue sur les scores (-3 â†’ 3) |
| **Pearson Correlation** | CorrÃ©lation entre prÃ©dictions et labels MOSI |

---

### 6ï¸âƒ£ EntraÃ®nement

HyperparamÃ¨tres clÃ©s :

- Optimizer : **AdamW**
- LR : **5e-5**
- Scheduler : **Cosine Annealing**
- Epochs : **80**
- Patience : **20**
- Batch size : **16**

Lâ€™entraÃ®nement inclut un **early stopping**, basÃ© sur la meilleure Acc-2 en validation.

---

### 7ï¸âƒ£ Ã‰valuation finale et sauvegarde

Ã€ la fin de lâ€™entraÃ®nement :

- Le meilleur modÃ¨le (selon **Acc-2** validation) est chargÃ©
- Les performances sont Ã©valuÃ©es sur le **test set**
- Le modÃ¨le final est sauvegardÃ© sous :



les poids du modÃ¨le
les mÃ©triques test
les statistiques de normalisation
la configuration du modÃ¨le
âž¡ï¸ nÃ©cessaire pour une infÃ©rence cohÃ©rente
Double-cliquez (ou appuyez sur EntrÃ©e) pour modifier

## ðŸ“Š RÃ©sultats â€” ModÃ¨le Baseline (BERT-base-uncased)

Nous avons entraÃ®nÃ© un premier modÃ¨le **baseline** en utilisant les embeddings texte provenant de  
**BERT-base-uncased**, combinÃ©s avec les embeddings audio (OpenSMILE) et vidÃ©o (OpenFace).  
Ce modÃ¨le utilise notre version simplifiÃ©e de **DEVANet** avec attention croisÃ©e multimodale.

Lâ€™entraÃ®nement sâ€™est arrÃªtÃ© automatiquement grÃ¢ce Ã  lâ€™**early stopping** Ã  lâ€™epoch 21.

### ðŸ”¥ Performances finales sur le Test Set

| Metric | Score |
|--------|--------|
| **Acc-2 (Binary)** | **0.8273** |
| **F1-weighted** | **0.8273** |
| **MAE** | **0.9959** |
| **Pearson Correlation** | **0.7374** |

âž¡ï¸ **Acc-2** et **F1-Weighted** au-dessus de **82%**.

### ðŸ“Œ Observations importantes

- Le modÃ¨le apprend rapidement, atteignant une prÃ©cision binaire de **94%** sur le train set avant rÃ©gularisation.
- Les rÃ©sultats en validation tournent autour de **0.73â€“0.75**, ce qui est cohÃ©rent avec MOSI.
- Le test set montre une bonne gÃ©nÃ©ralisation (Acc-2 â‰ˆ 0.8273).
- Le **MAE â‰ˆ 0.99** montre que le modÃ¨le reste stable pour de la rÃ©gression Ã©motionnelle continue.
- La **corrÃ©lation de Pearson â‰ˆ 0.74** indique une bonne cohÃ©rence entre labels rÃ©els et prÃ©dictions.

                   +--------------------------------+
                   |   1. TÃ©lÃ©chargement des donnÃ©es |
                   |     CMU-MOSI (vidÃ©os, audio,    |
                   |     transcriptions, labels)     |
                   +--------------------------------+
                                   |
                                   v
        +--------------------------------------------------------+
        | 2. Extraction & Encodage des caractÃ©ristiques          |
        |                                                        |
        |  ðŸ”¹ VISUEL : OpenFace                                  |
        |      â†’ landmarks, Action Units, embeddings             |
        |                                                        |
        |  ðŸ”¹ AUDIO : OpenSMILE + encodage audio                 |
        |      â†’ F0, jitter, shimmer, loudness, MFCC, etc.       |
        |      â†’ passage dans un encodeur pour obtenir           |
        |        un embedding audio fixe                         |
        |                                                        |
        |  ðŸ”¹ TEXTE : BERT-base-uncased                          |
        |      â†’ embeddings textuels 768d                        |
        +--------------------------------------------------------+
                                   |
                                   v
        +--------------------------------------------------------+
        |              3. Alignement temporel MOSI               |
        |  - Alignement audio/texte/vidÃ©o                        |
        |  - Segments synchronisÃ©s de longueur T=8               |
        +--------------------------------------------------------+
                                   |
                                   v
        +--------------------------------------------------------+
        |            4. PrÃ©traitement & Normalisation            |
        |  - Normalisation sÃ©parÃ©e par modalitÃ© (T, A, V)        |
        |  - Encodage du label :                                |
        |       â€¢ Valeur continue : [-3, 3]                      |
        |       â€¢ Label binaire : (score > 0)                    |
        |       â€¢ Label tri-class (Pos / Neg / Neutre)           |
        |  - Construction des DataLoaders                        |
        +--------------------------------------------------------+
                                   |
                                   v
        +--------------------------------------------------------+
        |      5. Fusion Multimodale (MFU â€“ Baseline)           |
        |  - Attention croisÃ©e Tâ†’A et Tâ†’V                        |
        |  - Pooling temporel (moyenne)                          |
        |  - Fusion T + A + V                                    |
        +--------------------------------------------------------+
                                   |
                                   v
        +--------------------------------------------------------+
        |                  6. ModÃ¨le Baseline                    |
        |  - DEVANet (couche dense)                              |
        |  - Sortie : prÃ©diction continue âˆˆ [-3, 3]              |
        +--------------------------------------------------------+
                                   |
                                   v
        +--------------------------------------------------------+
        |                 7. EntraÃ®nement                         |
        |  - HybridLoss (MSE + BCE binaire)                      |
        |  - AdamW + Early Stopping                              |
        |  - Suivi des mÃ©triques de validation                   |
        +--------------------------------------------------------+
                                   |
                                   v
        +--------------------------------------------------------+
        |           8. PrÃ©diction & Ã‰valuation finale            |
        |  - PrÃ©diction : score continu + classe binaire         |
        |  - MÃ©triques : Acc-2, F1-weighted, MAE, Pearson        |
        +--------------------------------------------------------+

  
## ðŸ” Variante RoBERTa (expÃ©rimentation supplÃ©mentaire)

En complÃ©ment de la baseline avec **BERT-base-uncased**, nous avons testÃ© une variante oÃ¹ :

- Les descriptions audio/vidÃ©o (prompts gÃ©nÃ©rÃ©s Ã  partir de loudness, jitter, shimmer, F0, etc.)  
  sont encodÃ©es avec **RoBERTa-base** au lieu de BERT.
- Un **TextEncoder transformer** (T = 8, d = 768) projette ces sorties en blocs de taille fixe.
- Les embeddings texte, audio et vidÃ©o sont ensuite alignÃ©s avec le fichier `label.csv`
  et injectÃ©s dans le mÃªme modÃ¨le **RegularizedDEVANet** (cross-modal attention + fusion).

Lâ€™entraÃ®nement et lâ€™Ã©valuation sont identiques Ã  la baseline (mÃªme split MOSI, mÃªmes mÃ©triques).

### ðŸ“Š RÃ©sultats (RoBERTa)

Sur le **test set**, nous obtenons environ :

- **Acc-2 (Binary)** â‰ˆ **0.74**
- (autres mÃ©triques dans le notebook dâ€™entraÃ®nement)

Ces rÃ©sultats sont **infÃ©rieurs** Ã  ceux de la baseline BERT-base-uncased  
(Acc-2 â‰ˆ 0.83), donc **la baseline BERT** reste notre modÃ¨le de rÃ©fÃ©rence officiel.

> ðŸ’¡ Le code complet de cette variante RoBERTa (encodage + alignement + entraÃ®nement)  
> est disponible dans le notebook du projet.

### 1. Meilleur encodeur texte

- Baseline : `bert-base-uncased` (BERT gÃ©nÃ©raliste).
- ModÃ¨le optimisÃ© : **`ayoubkirouane/BERT-Emotions-Classifier`**, un BERT prÃ©-entraÃ®nÃ© spÃ©cifiquement pour la classification des Ã©motions.
- Objectif : obtenir des embeddings textuels plus discriminants pour la valence.

### 2. Attention croisÃ©e multi-tÃªte stabilisÃ©e

- Baseline : attention croisÃ©e simple (une seule tÃªte, sans normalisation).
- ModÃ¨le optimisÃ© :
  - **Multi-Head Cross-Modal Attention** (4 tÃªtes) entre texteâ€“audio et texteâ€“vidÃ©o.
  - **Residual connection + LayerNorm** dans le bloc dâ€™attention.
- Objectif : mieux capturer les interactions fines entre modalitÃ©s et stabiliser lâ€™entraÃ®nement.

### 3. MFU (fusion multimodale) amÃ©liorÃ©

- Baseline : pooling temporel par moyenne seule, puis concatÃ©nation et projection.
- ModÃ¨le optimisÃ© :
  - Pooling **moyenne + max** pour chaque modalitÃ© (texte, audio, vidÃ©o).
  - Fusion via un bloc linÃ©aire + ReLU + LayerNorm.
- Objectif : garder Ã  la fois la tendance globale et les pics Ã©motionnels dans chaque sÃ©quence.

### 4. Fonction de perte hybride rÃ©ajustÃ©e

- Baseline :
  - `MSELoss` + `BCEWithLogitsLoss` avec pondÃ©ration 50% / 50%.
- ModÃ¨le optimisÃ© :
  - **`SmoothL1Loss` (Huber)** pour la partie rÃ©gression (valence continue).
  - `BCEWithLogitsLoss` avec **label smoothing** pour la partie binaire.
  - PondÃ©ration **30% MSE / 70% BCE**.
- Objectif : mieux Ã©quilibrer la rÃ©gression de la valence et la classification binaire (Acc-2), tout en rendant la perte moins sensible aux outliers.

### 5. Optimisation & entraÃ®nement

- Optimizer : toujours **AdamW**, mais avec :
  - `lr = 2e-5` (plus stable),
  - `weight_decay = 5e-4`,
  - `amsgrad=True` pour une meilleure convergence.
- Scheduler :
  - Baseline : `CosineAnnealingLR`.
  - ModÃ¨le optimisÃ© : **`CosineAnnealingWarmRestarts`** pour mieux explorer lâ€™espace de paramÃ¨tres.
- EntraÃ®nement :
  - **AMP (mixed precision)** avec `torch.cuda.amp` pour accÃ©lÃ©rer lâ€™entraÃ®nement et amÃ©liorer la stabilitÃ© numÃ©rique.
  - **Early stopping** plus agressif (`patience = 12`) pour limiter lâ€™overfitting.

### 6. Normalisation & rÃ©plicabilitÃ©

- Normalisation systÃ©matique des embeddings texte/audio/vidÃ©o Ã  partir des statistiques du train.
- Fixation dâ€™un **seed global (42)** pour PyTorch, NumPy et Python, afin de garantir la rÃ©plicabilitÃ© des rÃ©sultats.

## ðŸ“Š RÃ©sultats finaux sur le Test Set

AprÃ¨s lâ€™entraÃ®nement du modÃ¨le DEVANet optimisÃ© et la sÃ©lection du meilleur checkpoint
(basÃ© sur la mÃ©trique Acc-2 en validation), nous avons Ã©valuÃ© les performances sur le
jeu de test MOSI.

### ðŸ§ª MÃ©triques obtenues

| MÃ©trique                 | Valeur |
|--------------------------|--------|
| **Acc-2** (accuracy binaire) | **0.8363** |
| **F1-weighted**          | **0.8363** |
| **MAE** (erreur absolue moyenne) | **0.7624** |
| **CorrÃ©lation de Pearson** | **0.7757** |

### âœ… InterprÃ©tation des rÃ©sultats

- **Acc-2 = 83.63%** â†’ Le modÃ¨le discrimine efficacement les sentiments *positifs vs nÃ©gatifs*.  
- **F1-weighted â‰ˆ 0.836** â†’ Les performances sont Ã©quilibrÃ©es malgrÃ© le dÃ©sÃ©quilibre de classes.  
- **MAE â‰ˆ 0.76** â†’ Lâ€™erreur moyenne entre la prÃ©diction de sentiment continu et la vÃ©ritÃ© terrain reste faible.  
- **CorrÃ©lation de Pearson â‰ˆ 0.776** â†’ Le modÃ¨le suit bien la tendance de lâ€™intensitÃ© Ã©motionnelle rÃ©elle.

Ces rÃ©sultats montrent que **notre version optimisÃ©e de DEVANet** (attention multi-tÃªtes, pooling amÃ©liorÃ©, hybrid loss ajustÃ©e) obtient de meilleures performances que notre baseline.


               +--------------------------------+
               |   1. TÃ©lÃ©chargement des donnÃ©es |
               |     CMU-MOSI (vidÃ©o, audio,     |
               |     texte, labels [-3,3])       |
               +--------------------------------+
                               |
                               v
    +----------------------------------------------------------+
    | 2. Extraction & Encodage des caractÃ©ristiques             |
    |                                                          |
    |  ðŸ”¹ VISUEL : OpenFace                                    |
    |      â†’ AU (Action Units), landmarks, embeddings          |
    |                                                          |
    |  ðŸ”¹ AUDIO : OpenSMILE + encodeur audio                   |
    |      â†’ MFCC, loudness, jitter, shimmer, F0               |
    |      â†’ encodage â†’ vecteur audio fixe                     |
    |                                                          |
    |  ðŸ”¹ TEXTE : BERT amÃ©liorÃ©                                 |
    |      â†’ ModÃ¨le utilisÃ© :                                  |
    |            **"ayoubkirouane/BERT-Emotions-Classifier"**  |
    |      â†’ embeddings Ã©motionnels optimisÃ©s (768d)           |
    +----------------------------------------------------------+
                               |
                               v
    +----------------------------------------------------------+
    |            3. Alignement temporel (MOSi segmentÃ©)         |
    |  - Alignement Texte / Audio / VidÃ©o                       |
    |  - FenÃªtres synchronisÃ©es T = 8 frames                    |
    +----------------------------------------------------------+
                               |
                               v
    +----------------------------------------------------------+
    |    4. PrÃ©traitement & Normalisation multimodale          |
    |  - Normalisation indÃ©pendante (texte, audio, vidÃ©o)      |
    |  - Augmentation lÃ©gÃ¨re : ajout de bruit                  |
    |  - Encodage du label :                                   |
    |        â€¢ Valeur continue âˆˆ [-3,3]                        |
    |        â€¢ Binaire : (label > 0)                           |
    |        â€¢ Tri-class (Pos / Neg / Neu)                     |
    +----------------------------------------------------------+
                               |
                               v
    +----------------------------------------------------------+
    |     5. Fusion Multimodale AmÃ©liorÃ©e (Optimized MFU)     |
    |                                                          |
    |  ðŸ”¸ Multi-Head Cross-Modal Attention                      |
    |       â€¢ 4 tÃªtes parallÃ¨le pour chaque modalitÃ©           |
    |       â€¢ T â†’ A et T â†’ V                                   |
    |                                                          |
    |  ðŸ”¸ Pooling avancÃ© : mean + max                          |
    |       â†’ meilleure capture des pics Ã©motionnels           |
    |                                                          |
    |  ðŸ”¸ Fusion : concat(T, A, V) â†’ couche dense              |
    +----------------------------------------------------------+
                               |
                               v
    +----------------------------------------------------------+
    |             6. ModÃ¨le OptimisÃ© : DEVANetOptim             |
    |  - MFU amÃ©liorÃ© + classifier profond                     |
    |  - Sortie : score Ã©motionnel continu âˆˆ [-3,3]            |
    +----------------------------------------------------------+
                               |
                               v
    +----------------------------------------------------------+
    |         7. EntraÃ®nement OptimisÃ©                         |
    |  ðŸ”¹ Loss hybrides amÃ©liorÃ©e                               |
    |       â€¢ SmoothL1 (MSE robuste)                            |
    |       â€¢ BCE avec label smoothing                          |
    |       â€¢ pondÃ©ration (0.3/0.7) optimisÃ©e                   |
    |                                                          |
    |  ðŸ”¹ Optimisateur : AdamW + amsgrad                        |
    |  ðŸ”¹ Mixed precision : AMP (autocast + GradScaler)         |
    |  ðŸ”¹ Scheduler : CosineAnnealingWarmRestarts               |
    |  ðŸ”¹ Gradient clipping : 0.5                               |
    +----------------------------------------------------------+
                               |
                               v
    +----------------------------------------------------------+
    |         8. Ã‰valuation & Sauvegarde du meilleur modÃ¨le    |
    |  - MÃ©triques : Acc-2, F1-weighted, MAE, Pearson          |
    |  - Early stopping                                         |
    |  - Sauvegarde : devanet_optimized_final.pth               |
    |    â†’ inclut normalisation + mÃ©triques + poids            |
    +----------------------------------------------------------+
